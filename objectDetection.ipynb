{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b3CSpusrfuIa"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def image_to_array(image_path):\n",
        "    image = Image.open(image_path)\n",
        "    image = image.convert('L')  # Convert to grayscale\n",
        "    image = image.resize((120, 120))  # Resize image\n",
        "    image_array = np.array(image) / 255.0  # Normalize pixel values\n",
        "    return np.expand_dims(image_array, axis=0)  # Add channel dimension\n"
      ],
      "metadata": {
        "id": "_JK5JGeofxqL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvLayer:\n",
        "    def __init__(self, num_filters, kernel_size, stride=1, padding=0):\n",
        "        self.num_filters = num_filters\n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "        self.filters = np.random.randn(num_filters, kernel_size, kernel_size) * 0.1\n",
        "\n",
        "    def _apply_padding(self, input_image):\n",
        "        if self.padding == 0:\n",
        "            return input_image\n",
        "        else:\n",
        "            return np.pad(input_image, ((0, 0), (self.padding, self.padding), (self.padding, self.padding)), mode='constant')\n",
        "\n",
        "    def forward(self, input_image):\n",
        "        self.input_image = input_image\n",
        "        input_image = self._apply_padding(input_image)\n",
        "        num_channels, h, w = input_image.shape\n",
        "        output_dim = ((h - self.kernel_size) // self.stride) + 1\n",
        "        output = np.zeros((self.num_filters, output_dim, output_dim))\n",
        "\n",
        "        for f in range(self.num_filters):\n",
        "            filter = self.filters[f]\n",
        "            for i in range(0, output_dim):\n",
        "                for j in range(0, output_dim):\n",
        "                    region = input_image[:, i * self.stride:i * self.stride + self.kernel_size, j * self.stride:j * self.stride + self.kernel_size]\n",
        "                    output[f, i, j] = np.sum(region * filter)\n",
        "        return output\n",
        "\n",
        "    def backward(self, dvalues, learning_rate):\n",
        "        for i in range(self.num_filters):\n",
        "            self.filters[i] -= learning_rate * dvalues[i]\n"
      ],
      "metadata": {
        "id": "D6IGWh_7f1t_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MaxPoolingLayer:\n",
        "    def __init__(self, pool_size=2, stride=2):\n",
        "        self.pool_size = pool_size\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, input_image):\n",
        "        num_filters, h, w = input_image.shape\n",
        "        output_dim = ((h - self.pool_size) // self.stride) + 1\n",
        "        output = np.zeros((num_filters, output_dim, output_dim))\n",
        "\n",
        "        for f in range(num_filters):\n",
        "            for i in range(0, h - self.pool_size + 1, self.stride):\n",
        "                for j in range(0, w - self.pool_size + 1, self.stride):\n",
        "                    region = input_image[f, i:i + self.pool_size, j:j + self.pool_size]\n",
        "                    output[f, i // self.stride, j // self.stride] = np.max(region)\n",
        "\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "uL19y1WHf49x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Activation_ReLU:\n",
        "    def forward(self, inputs):\n",
        "        self.output = np.maximum(0, inputs)\n"
      ],
      "metadata": {
        "id": "HhPYieNbf7NH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FlattenLayer:\n",
        "    def forward(self, inputs):\n",
        "        self.output = inputs.flatten().reshape(1, -1)\n"
      ],
      "metadata": {
        "id": "0-IuiFuAf84p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Layer_dense:\n",
        "    def __init__(self, inputs, neurons):\n",
        "        self.weights = np.random.randn(inputs, neurons) * np.sqrt(2. / (inputs + neurons)).astype(np.float32)\n",
        "        self.biases = np.zeros((1, neurons), dtype=np.float32)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        self.input = inputs\n",
        "        self.output = np.dot(inputs, self.weights) + self.biases\n",
        "\n",
        "    def backward(self, dvalues, learning_rate, max_grad_norm=1.0):\n",
        "        dweights = np.dot(self.input.T, dvalues)\n",
        "        dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
        "\n",
        "        # Clip gradients to avoid explosion\n",
        "        dweights = np.clip(dweights, -max_grad_norm, max_grad_norm)\n",
        "        dbiases = np.clip(dbiases, -max_grad_norm, max_grad_norm)\n",
        "\n",
        "        self.weights -= learning_rate * dweights\n",
        "        self.biases -= learning_rate * dbiases\n"
      ],
      "metadata": {
        "id": "3O2ni3kif-qF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class OutputLayer:\n",
        "    def __init__(self, inputs):\n",
        "        self.weights = np.random.randn(inputs, 1) * 0.01\n",
        "        self.biases = np.zeros((1, 1), dtype=np.float32)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        self.output = np.dot(inputs, self.weights) + self.biases\n",
        "\n",
        "    def backward(self, dvalues, learning_rate):\n",
        "        dweights = np.dot(self.output.T, dvalues)\n",
        "        dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
        "        self.weights -= learning_rate * dweights\n",
        "        self.biases -= learning_rate * dbiases\n"
      ],
      "metadata": {
        "id": "RpAQxqdRgAgt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mean Squared Error Loss\n",
        "class MeanSquaredError:\n",
        "    def forward(self, y_pred, y_true):\n",
        "        print(y_pred, y_true)\n",
        "        return np.mean((y_pred - y_true) ** 2)\n",
        "\n",
        "    def backward(self, y_pred, y_true):\n",
        "        return 2 * (y_pred - y_true) / y_true.size\n"
      ],
      "metadata": {
        "id": "_FXHfsvfgC3U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_weights(epoch):\n",
        "    weights = {\n",
        "        \"conv1\": conv1.filters,\n",
        "        \"conv2\": conv2.filters,\n",
        "        \"dense1_weights\": dense1.weights,\n",
        "        \"dense1_biases\": dense1.biases,\n",
        "        \"output_weights\": output_layer.weights,\n",
        "        \"output_biases\": output_layer.biases,\n",
        "    }\n",
        "    filename = f'weights_epoch_{epoch}.pkl'\n",
        "    with open(filename, 'wb') as f:\n",
        "        pickle.dump(weights, f)\n",
        "    print(f\"Weights saved to {filename}\")\n",
        "\n",
        "def load_weights(epoch):\n",
        "    filename = f'weights_epoch_{epoch}.pkl'\n",
        "    try:\n",
        "        with open(filename, 'rb') as f:\n",
        "            weights = pickle.load(f)\n",
        "        conv1.filters = weights[\"conv1\"]\n",
        "        conv2.filters = weights[\"conv2\"]\n",
        "        dense1.weights = weights[\"dense1_weights\"]\n",
        "        dense1.biases = weights[\"dense1_biases\"]\n",
        "        output_layer.weights = weights[\"output_weights\"]\n",
        "        output_layer.biases = weights[\"output_biases\"]\n",
        "        print(f\"Weights loaded from {filename}\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"No weights found for epoch {epoch}. Starting fresh.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "owUyny8ygJHx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_path = 'img_count_45.jpeg'\n",
        "image_array = image_to_array(image_path)\n",
        "target_output = np.array([[0.45]])  # True count\n",
        "\n",
        "# Calculate flattened input size dynamically\n",
        "input_image_size = 120"
      ],
      "metadata": {
        "id": "WqCb8rAwgOXr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conv1_output_size = (input_image_size - 3 + 2 * 1) // 1 + 1  # After Conv1\n",
        "pool1_output_size = (conv1_output_size - 2) // 2 + 1         # After Pool1\n",
        "conv2_output_size = (pool1_output_size - 3 + 2 * 1) // 1 + 1 # After Conv2\n",
        "pool2_output_size = (conv2_output_size - 2) // 2 + 1         # After Pool2\n",
        "flatten_input_size = 64 * pool2_output_size * pool2_output_size  # 64 filters\n",
        "\n",
        "# Initialize Layers\n",
        "conv1 = ConvLayer(num_filters=32, kernel_size=3, stride=1, padding=1)\n",
        "pool1 = MaxPoolingLayer(pool_size=2, stride=2)\n",
        "activation1 = Activation_ReLU()\n",
        "conv2 = ConvLayer(num_filters=64, kernel_size=3, stride=1, padding=1)\n",
        "pool2 = MaxPoolingLayer(pool_size=2, stride=2)\n",
        "activation2 = Activation_ReLU()\n",
        "flatten = FlattenLayer()\n",
        "\n",
        "dense1 = Layer_dense(flatten_input_size, 128)\n",
        "output_layer = OutputLayer(128)\n",
        "mse_loss = MeanSquaredError()"
      ],
      "metadata": {
        "id": "OFVfLq1kgPE_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_epoch = 0\n",
        "saved_files = [f for f in os.listdir() if f.startswith(\"weights_epoch_\")]\n",
        "if saved_files:\n",
        "    latest_file = max(saved_files, key=lambda x: int(x.split('_')[-1].split('.')[0]))\n",
        "    start_epoch = int(latest_file.split('_')[-1].split('.')[0])\n",
        "    load_weights(start_epoch)\n"
      ],
      "metadata": {
        "id": "QgwQXzwngTn6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(image_array, target_output, epochs=25, learning_rate=1e-5):\n",
        "    for epoch in range(epochs):\n",
        "        # Forward pass\n",
        "        conv_output1 = conv1.forward(image_array)\n",
        "        activation1.forward(conv_output1)\n",
        "        pool_output1 = pool1.forward(activation1.output)\n",
        "        conv_output2 = conv2.forward(pool_output1)\n",
        "        activation2.forward(conv_output2)\n",
        "        pool_output2 = pool2.forward(activation2.output)\n",
        "        flatten.forward(pool_output2)\n",
        "        dense1.forward(flatten.output)\n",
        "        output_layer.forward(dense1.output)\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = mse_loss.forward(output_layer.output, target_output)\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss}\")\n",
        "\n",
        "        predicted_count = output_layer.output[0][0]  # Since output is a single value, we take the first element\n",
        "        print(f\"Predicted count: {predicted_count}\")\n",
        "\n",
        "        # Backward pass\n",
        "        dvalues = mse_loss.backward(output_layer.output, target_output)\n",
        "        output_layer.backward(dvalues, learning_rate)\n",
        "        dense1.backward(dvalues, learning_rate)\n",
        "\n",
        "        # Save weights after each epoch\n",
        "        save_weights(epoch + 1)\n",
        "\n",
        "\n",
        "# Input Image Processing\n",
        "\n",
        "\n",
        "# Train the model\n",
        "train_model(image_array, target_output, epochs=25, learning_rate=1e-5)\n"
      ],
      "metadata": {
        "id": "BJnwLWRSgVvC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}